{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树\n",
    "\n",
    "## 问题1：\n",
    "* 什么是决策树？\n",
    "\n",
    "决策树算法是一种归纳分类算法,它通过对训练集的学习,挖掘出有用的规则,用于对新集进行预测。\n",
    "\n",
    "决策树算法采用树形结构，自顶向下递归方式构造决策树。\n",
    "\n",
    "决策树由下面几种元素构成：\n",
    "\n",
    "![title](other/tree2.png)\n",
    "\n",
    "1. 根节点：包含所有的样本；\n",
    "2. 内部节点：对应样本特征属性测试；\n",
    "3. 分支：样本测试的结果；\n",
    "3. 叶子节点：代表决策的结果。\n",
    "\n",
    "## 问题2：\n",
    "* 如何构造决策数？\n",
    "\n",
    "观察下面关于判断猫咪是否是哺乳动物来的决策树。\n",
    "\n",
    "![title](other/案例.jpg)\n",
    "\n",
    "1.构造\n",
    "\n",
    "什么是构造呢？构造就是生成一棵完整的决策树。简单来说， 构造的过程就是选择什么属性作为节点的过程 ，那么在构造过程中，会存在三种节点：\n",
    "\n",
    "1）根节点：就是树的最顶端，最开始的那个节点，如上图的“体温”。\n",
    "\n",
    "2）内部节点：就是树中间的那些节点，如上图的“胎生”；\n",
    "\n",
    "3）叶节点：就是树最底部的节点，也就是决策结果，如上图的“哺乳动物”，“非哺乳动物”。\n",
    "\n",
    "节点之间存在父子关系。比如根节点会有子节点，子节点会有子子节点，但是到了叶节点就停止了，叶节点不存在子节点。那么在构造过程中，你要解决三个重要的问题：\n",
    "\n",
    "1）选择哪个属性作为根节点；\n",
    "\n",
    "2）选择哪些属性作为子节点；\n",
    "\n",
    "3）什么时候停止并得到目标状态，即叶节点。\n",
    "\n",
    "# 例子：\n",
    "\n",
    "下面我们通过一个例子对决策树的构造进行具体的讲解。\n",
    "我们现在有这样一个数据集：\n",
    "\n",
    "![title](other/打球例子.png)\n",
    "\n",
    "我们该如何构造一个判断是否去打篮球的决策树呢？再回顾一下决策树的构造原理，在决策过程中有三个重要的问题：将哪个属性作为根节点？选择哪些属性作为后继节点？什么时候停止并得到目标值？\n",
    "\n",
    "显然将哪个属性（天气、温度、湿度、刮风）作为根节点是个关键问题，在这里我们先介绍两个指标：纯度和信息熵。\n",
    "\n",
    "先来说一下纯度。你可以把决策树的构造过程理解成为寻找纯净划分的过程。数学上，我们可以用纯度来表示，纯度换一种方式来解释就是让目标变量的分歧最小。\n",
    "\n",
    "我在这里举个例子，假设有 3 个集合：\n",
    "\n",
    "集合 1：6 次都去打篮球；\n",
    "\n",
    "集合 2：4 次去打篮球，2 次不去打篮球；\n",
    "\n",
    "集合 3：3 次去打篮球，3 次不去打篮球。\n",
    "\n",
    "按照纯度指标来说，集合 1> 集合 2> 集合 3。因为集合 1 的分歧最小，集合 3 的分歧最大。\n",
    "\n",
    "## “信息熵”：\n",
    "\n",
    "然后我们再来介绍信息熵（entropy）的概念。\n",
    "\n",
    "![title](other/香农.png)\n",
    "\n",
    "1948 年， 香农 提出了“信息熵(shāng)” 的概念，解决了对信息的量化度量问题。\n",
    "一条信息的信息量大小和它的不确定性有直接的关系。比如说，我们要搞清楚一件非常非常不确定的事，或是我们一无所知的事情，就需要了解大量的信息。相反，如果我们对某件事已经有了较多的了解，我们不需要太多的信息就能把它搞清楚。所以，从这个角度，我们可以认为，信息量的度量就等于不确定性的多少。\n",
    "\n",
    "公式：\n",
    "\n",
    "![title](other/公式.png)\n",
    "\n",
    "变量的不确定性越大，熵也就越大，因此我们定义的“纯度”就代表信息量的多少，我们通过信息熵就可以对各个节点进行构造了。\n",
    "\n",
    "我举个简单的例子，假设有 2 个集合\n",
    "\n",
    "集合 1：5 次去打篮球，1 次不去打篮球；\n",
    "\n",
    "集合 2：3 次去打篮球，3 次不去打篮球。\n",
    "\n",
    "在集合 1 中，有 6 次决策，其中打篮球是 5 次，不打篮球是 1 次。那么假设：类别 1 为“打篮球”，即次数为 5；类别 2 为“不打篮球”，即次数为 1。那么节点划分为类别 1 的概率是 5/6，为类别 2 的概率是 1/6，带入上述信息熵公式可以计算得出：\n",
    "\n",
    "![title](other/公式1.png)\n",
    "\n",
    "同样，集合 2 中，也是一共 6 次决策，其中类别 1 中“打篮球”的次数是 3，类别 2“不打篮球”的次数也是 3，那么信息熵为多少呢？我们可以计算得出：\n",
    "\n",
    "![title](other/公式2.png)\n",
    "\n",
    "从上面的计算结果中可以看出，信息熵越大，纯度越低。当集合中的所有样本均匀混合时，信息熵最大，纯度最低。\n",
    "\n",
    "我们在构造决策树的时候，会基于纯度来构建。而经典的 “不纯度”的指标有三种，分别是信息增益（ID3 算法）、信息增益率（C4.5 算法）以及基尼指数（Cart 算法）。\n",
    "\n",
    " \n",
    "\n",
    "1.ID3\n",
    "\n",
    "ID3 算法计算的是 信息增益 ，信息增益指的就是划分可以带来纯度的提高，信息熵的下降。\n",
    "\n",
    "在计算的过程中，我们会计算每个子节点的归一化信息熵，即按照每个子节点在跟节点中出现的概率，来计算这些子节点的信息熵。所以信息增益的公式可以表示为：\n",
    "\n",
    "* 它的计算公式是：父节点的信息熵减去所有子节点的信息熵。\n",
    "\n",
    "![title](other/公式3.png)\n",
    "\n",
    "公式中 D 是根节点，Di 是子节点，Gain(D,a) 中的 a 作为 D 节点的属性选择。\n",
    "\n",
    "假设天气 = 晴的时候，会有 5 次去打篮球，5 次不打篮球。其中 D1 刮风 = 是，有 2 次打篮球，1 次不打篮球。D2 刮风 = 否，有 3 次打篮球，4 次不打篮球。那么 a 代表节点的属性，即天气 = 晴。\n",
    "\n",
    "针对这个例子，D 作为节点的信息增益为：\n",
    "\n",
    "![title](other/公式4.png)\n",
    "\n",
    "也就是 D 节点的信息熵 -2 个子节点的归一化信息熵。2 个子节点归一化信息熵 =3/10 的 D1 信息熵 +7/10 的 D2 信息熵。\n",
    "\n",
    "我们基于 ID3 的算法规则，完整地计算下我们的训练集，训练集中一共有 7 条数据，3 个打篮球，4 个不打篮球，所以根节点的信息熵是：\n",
    "\n",
    "![title](other/公式5.png)\n",
    "\n",
    "如果你将天气作为属性的划分，会有三个叶子节点 D1、D2 和 D3，分别对应的是晴天、阴天和小雨。我们用 + 代表去打篮球，- 代表不去打篮球。于是我们可以用下面的方式来记录 D1，D2，D3： \n",
    "\n",
    "D1(天气 = 晴天)={1+,2-}\n",
    "\n",
    "D2(天气 = 阴天)={1+,1-}\n",
    "\n",
    "D3(天气 = 小雨)={1+,1-}\n",
    "\n",
    "我们先分别计算三个叶子节点的信息熵：\n",
    "\n",
    "![title](other/公式6.png)\n",
    "\n",
    "因为 D1 有 3 个记录，D2 有 2 个记录，D3 有 2 个记录，所以 D 中的记录一共是 3+2+2=7，即总数为 7。所以 D1 在 D（根节点）中的概率是 3/7，D2 在根节点的概率是 2/7，D3 在根节点的概率是 2/7。那么作为子节点的归一化信息熵 = 3/7*0.918+2/7*1.0+2/7*1.0=0.965。\n",
    "\n",
    "因为我们用 ID3 中的信息增益来构造决策树，所以要计算每个节点的信息增益。\n",
    "\n",
    "天气属性节点的信息增益 = 根节点信息熵 - 子节点信息熵： \n",
    "\n",
    "Gain(D , 天气)=0.985-0.965=0.020。\n",
    "\n",
    "同理我们可以计算出其他属性作为根节点的信息增益，它们分别为 ：\n",
    "\n",
    "Gain(D , 温度)=0.128\n",
    "\n",
    "Gain(D , 湿度)=0.020\n",
    "\n",
    "Gain(D , 刮风)=0.020\n",
    "\n",
    "我们能看出来温度作为属性的信息增益最大。因为 ID3 就是要将信息增益最大的节点作为父节点，这样可以得到纯度高的决策树，所以我们将温度作为根节点。\n",
    "\n",
    "然后我们要将第一个叶节点，也就是 D1={1-,2-,3+,4+}进一步进行分裂，往下划分，计算其不同属性（天气、湿度、刮风）作为节点的信息增益，可以得到：\n",
    "\n",
    "Gain(D , 天气)=0\n",
    "\n",
    "Gain(D , 湿度)=0\n",
    "\n",
    "Gain(D , 刮风)=0.0615\n",
    "\n",
    "我们能看到刮风为 D1 的节点都可以得到最大的信息增益，这里我们选取刮风作为节点。同理，我们可以按照上面的计算步骤得到完整的决策树。\n",
    "\n",
    "下面我们就来构建这颗决策树！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 样本数据集\n",
    "还是使用上面这个例子。我们的数据集有10个样本，每个样本有各自的特征['天气','温度','湿度','刮风']，每个特征有各自的属性，比如特征“天气”有属性[“晴”，“雨”，“阴”]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=[\n",
    "            ['晴','热','高','否','不打球'],\n",
    "            ['晴','热','高','是','不打球'],\n",
    "            ['阴','热','高','否','打球'],\n",
    "            ['雨','温','高','否','打球'],\n",
    "            ['雨','凉爽','中','否','打球'],\n",
    "            ['雨','凉爽','中','是','不打球'],\n",
    "            ['阴','凉爽','中','是','打球'],\n",
    "            ['晴','温','高','否','不打球'],\n",
    "            ['晴','凉爽','中','否','打球'],\n",
    "            ['雨','温','中','否','打球'],\n",
    "            ]\n",
    "dataLabels = ['天气','温度','湿度','刮风']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构造信息熵计算方法\n",
    "输入样本，使用math数学计算模块输出信息熵值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def calcShannonEnt(dataSet):\n",
    "    #样本总个数，这里为10个\n",
    "    totalNum = len(dataSet)\n",
    "    #类别集合\n",
    "    labelSet = {}\n",
    "    #计算每个特征的样本个数\n",
    "    for dataVec in dataSet:\n",
    "        \n",
    "        # 取出样本最后一列的标签'是否打球'\n",
    "        label = dataVec[-1]\n",
    "        \n",
    "        # 将label添加到labelSet集合,记录“打球”和“不打球”的数量，最后labelSet为：{'不打球': 4, '打球': 6}\n",
    "        if label not in labelSet.keys():\n",
    "            labelSet[label] = 0\n",
    "        labelSet[label] += 1\n",
    "        \n",
    "    shannonEnt = 0\n",
    "    # 根据学习的公式计算熵值\n",
    "    for key in labelSet:\n",
    "        pi = float(labelSet[key])/totalNum\n",
    "        shannonEnt -= pi*math.log(pi,2)\n",
    "    return shannonEnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 划分样本\n",
    "输入dataSet（样本集），featNum（特征），featvalue（特征属性）\n",
    "输出与当前特征属性相关的其他特征属性，为计算子节点的熵值做准备\n",
    "\n",
    "* 比如当前特征属性是“晴”，那么输出为： [['热', '高', '否', '不打球'], ['热', '高', '是', '不打球'], ['温', '高', '否', '不打球'], ['凉爽', '中', '否', '打球']]\n",
    "\n",
    "* 这其实就是计算子节点的熵值，当“天气”为父节点，那么先计算父节点下各个属性的熵值，比如先计算“晴”的熵值，就需要直到与“晴”相关的样本有哪些。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDataSet(dataSet, featNum, featvalue):\n",
    "    retDataSet = []\n",
    "    for dataVec in dataSet:\n",
    "        if dataVec[featNum] == featvalue:\n",
    "            splitData = dataVec[:featNum]\n",
    "            splitData.extend(dataVec[featNum+1:])\n",
    "            retDataSet.append(splitData)\n",
    "    return retDataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算信息增益\n",
    "* 输入为dataset数据集；\n",
    "* 输出为熵值最大的特征索引，如果最大熵值特征为'天气'，在dataLabels中的索引就为0。\n",
    "\n",
    "方法：\n",
    "1. 通过信息熵计算公式计算样本信息熵，“打球”和“不打球”的熵值。\n",
    "\n",
    "![title](other/公式.png)   \n",
    "\n",
    "2. 通过ID3算法计算公式计算每个特征与“打球”和“不打球”的信息增益。\n",
    "\n",
    "![title](other/公式3.png)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chooseBestFeatToSplit(dataSet):\n",
    "    \n",
    "    # 计算总的特征个数，输出 4个\n",
    "    featNum = len(dataSet[0]) - 1\n",
    "    \n",
    "    # 设定信息增益\n",
    "    maxInfoGain = 0\n",
    "    bestFeat = -1\n",
    "    \n",
    "    # 输入样本，计算样本熵值。\n",
    "    baseShanno = calcShannonEnt(dataSet)\n",
    "    \n",
    "    # 对每一个特征进行分类，找出信息增益最大的特征\n",
    "    for i in range(featNum):\n",
    "        \n",
    "        # 返回每一行的数据。\n",
    "        featList = [dataVec[i] for dataVec in dataSet]\n",
    "        \n",
    "        # 找出特征属性，如'天气'的属性：“晴”，“阴”，“雨”\n",
    "        featList = set(featList)\n",
    "        \n",
    "        newShanno = 0\n",
    "        \n",
    "        # 通过熵值公式计算以第i个特征进行分类后的熵值，计算子节点熵值。如“晴”在“天气”中的熵值。\n",
    "        for featValue in featList:\n",
    "            subDataSet = splitDataSet(dataSet, i, featValue)\n",
    "            prob = len(subDataSet)/float(len(dataSet))\n",
    "            newShanno += prob*calcShannonEnt(subDataSet)\n",
    "            \n",
    "        #ID3算法：计算信息增益,通过父节点的信息熵减去所有子节点的信息熵\n",
    "        infoGain = baseShanno - newShanno\n",
    "        \n",
    "        #C4.5算法：计算信息增益比\n",
    "        #infoGain = (baseShanno - newShanno)/baseShanno\n",
    "        \n",
    "        #找出信息增益值最大的对应特征\n",
    "        if infoGain > maxInfoGain:\n",
    "            maxInfoGain = infoGain\n",
    "            bestFeat = i\n",
    "    return bestFeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建决策树\n",
    "\n",
    "* 输入样本和特征；\n",
    "* 通过调用之前创建的各个方法，输出分类结果，以字典的形式展现出来。\n",
    "\n",
    "方法：\n",
    "1. 通过计算每个特征的信息增益，确定父节点和所有子节点在决策树中的位置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建决策树\n",
    "def createDecideTree(dataSet, featName):\n",
    "    #数据集的分类类别，“打球”，“不打球”\n",
    "    classList = [dataVec[-1] for dataVec in dataSet]\n",
    "    #所有样本属于同一类时，停止划分，也就是说当前叶节点无法再分时，返回当前类别\n",
    "    if len(classList) == classList.count(classList[0]):\n",
    "        \n",
    "        return classList[0]\n",
    "    #所有特征已经遍历完，停止划分，返回样本数最多的类别\n",
    "    if len(dataSet[0]) == 1:\n",
    "        return majorityCnt(classList)\n",
    "    \n",
    "    #选择最好的特征进行划分\n",
    "    bestFeat = chooseBestFeatToSplit(dataSet)\n",
    "    beatFestName = featName[bestFeat]\n",
    "    print('可建分支特征有：', beatFestName)\n",
    "    \n",
    "    # 如果当前特征“bestFeat”被划分好了就将这个特征从特征集“dataLabels”中删除，不再参加计算。\n",
    "    del featName[bestFeat]\n",
    "    \n",
    "    #以字典形式表示决策树\n",
    "    DTree = {beatFestName:{}}\n",
    "    \n",
    "    #根据选择的特征，遍历该特征的所有属性值，再使用createDecideTree()函数划分叶节点。\n",
    "    featValue = [dataVec[bestFeat] for dataVec in dataSet]\n",
    "    \n",
    "    # 将对应特征的属性按照信息增益大小排列，比如以天气特征排列，就输出{'晴', '雨', '阴'}\n",
    "    featValue = set(featValue)\n",
    "    \n",
    "    # 将每个特征都进行以上操作，直到不能再继续分类为止。\n",
    "    for value in featValue:\n",
    "        subFeatName = featName[:]\n",
    "        DTree[beatFestName][value] = createDecideTree(splitDataSet(dataSet,bestFeat,value), subFeatName)\n",
    "    return DTree\n",
    "\n",
    "myTree = createDecideTree(dataset,dataLabels)\n",
    "print(\"决策树模型：\")\n",
    "print(myTree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 可以看到“湿度”特征没有被放入决策树中，因为“湿度”特征的信息增益是最小的，没有对最终的分类做出贡献，并且可能会影响决策树的分类效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用树形结构展现决策树\n",
    "通过\"tree_plot\"模块画出树状图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tree_plot\n",
    "tree_plot.createPlot(myTree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 进行预测\n",
    "方法：\n",
    "* 用测试样本在决策树中找出属性一致的叶节点作为类别。 \n",
    "\n",
    "例如当前测试样本为：['晴','温','中','是'] ，判断这个样本情况下是否打球。\n",
    "\n",
    "1. 根据决策树可以知道根节点为特征“天气”；\n",
    "2. 在样本中找出对应的特征属性并按信息增益的大小排序分别为[“天气”：“晴”，“温度”：“温”，“刮风”：“是”]\n",
    "3. 按照信息增益大小的顺序在决策树中进行对比，一旦找到叶节点就做出分类，比如在当前样本中“温度”特征的属性为“温”，因此类别直接判断为“不打球”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def classify(tree,feat,featValue):\n",
    "    \n",
    "    # 在构建好的决策树中找出父节点。\n",
    "    firstFeat = list(tree.keys())[0]\n",
    "    \n",
    "    # 找出当前父节点特征下的所有子节点。\n",
    "    secondDict = tree[firstFeat]\n",
    "    \n",
    "    # 父节点在给定特征列表中的位置索引。\n",
    "    featIndex = feat.index(firstFeat)\n",
    "    \n",
    "    # 不断的在决策树中做判断，判断当前样本属性是否与决策中的属性相等，直到找到叶节点输出类别。\n",
    "    for key in secondDict.keys():\n",
    "        if featValue[featIndex] == key:\n",
    "            if type(secondDict[key]).__name__ == 'dict':\n",
    "                classLabel = classify(secondDict[key],feat,featValue)\n",
    "            else:\n",
    "                classLabel = secondDict[key]\n",
    "    return classLabel\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 输入4组数据进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = ['天气','温度','湿度','刮风']\n",
    "dataSet2 = [ ['晴','温','中','是'],\n",
    "                ['阴','温','高','是'],\n",
    "                ['阴','热','中','否'],\n",
    "                ['雨','凉爽','高','否'],]\n",
    "\n",
    "print(\"预测结果：\")\n",
    "for dataVec2 in dataSet2:\n",
    "    print(classify(myTree,feat,dataVec2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
